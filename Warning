UserWarning: ‚Å†‚ÄØas_target_tokenizer‚ÄØ‚Å† is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument ‚Å†‚ÄØtext_target‚ÄØ‚Å† of the regular ‚Å†‚ÄØ__call__‚ÄØ‚Å† method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call
FutureWarning: ‚Å†‚ÄØevaluation_strategy‚ÄØ‚Å† is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use ‚Å†‚ÄØeval_strategy‚ÄØ‚Å† instead

Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}

Larger datasets often result in lower validation losses because the model has more diverse examples to learn from. Smaller datasets can lead to higher validation losses because the model may struggle to generalize well.
